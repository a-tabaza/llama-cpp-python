{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0aea66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d31368dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp.llama_chat_format import Llava15ChatHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3013a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip_init: loaded meta data with 24 key-value pairs and 520 tensors from /home/atabaza/Desktop/qwen_vl_ocr_quantized/weights/Qwen2.5-VL-3B-Instruct-mmproj-f16.gguf\n",
      "clip_init: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "clip_init: - kv   0:                       general.architecture str              = clip\n",
      "clip_init: - kv   1:                        general.description str              = image encoder for Qwen2VL\n",
      "clip_init: - kv   2:                          general.file_type u32              = 1\n",
      "clip_init: - kv   3:                      clip.has_text_encoder bool             = false\n",
      "clip_init: - kv   4:                    clip.has_vision_encoder bool             = true\n",
      "clip_init: - kv   5:                    clip.has_qwen2vl_merger bool             = true\n",
      "clip_init: - kv   6:                        clip.projector_type str              = qwen2vl_merger\n",
      "clip_init: - kv   7:                              clip.use_silu bool             = true\n",
      "clip_init: - kv   8:                              clip.use_gelu bool             = false\n",
      "clip_init: - kv   9:                           clip.use_glu_mlp bool             = true\n",
      "clip_init: - kv  10:                          clip.use_rms_norm bool             = true\n",
      "clip_init: - kv  11:          clip.vision.fullatt_block_indexes arr[i32,4]       = [7, 15, 23, 31]\n",
      "clip_init: - kv  12:                    clip.vision.window_size u32              = 112\n",
      "clip_init: - kv  13:               clip.vision.embedding_length u32              = 1280\n",
      "clip_init: - kv  14:                 clip.vision.projection_dim u32              = 2048\n",
      "clip_init: - kv  15:                     clip.vision.patch_size u32              = 14\n",
      "clip_init: - kv  16:                     clip.vision.image_size u32              = 560\n",
      "clip_init: - kv  17:           clip.vision.attention.head_count u32              = 16\n",
      "clip_init: - kv  18:   clip.vision.attention.layer_norm_epsilon f32              = 0.000001\n",
      "clip_init: - kv  19:                    clip.vision.block_count u32              = 32\n",
      "clip_init: - kv  20:            clip.vision.feed_forward_length u32              = 0\n",
      "clip_init: - kv  21:                               general.name str              = qwen7\n",
      "clip_init: - kv  22:                     clip.vision.image_mean arr[f32,3]       = [0.481455, 0.457828, 0.408211]\n",
      "clip_init: - kv  23:                      clip.vision.image_std arr[f32,3]       = [0.268630, 0.261303, 0.275777]\n",
      "clip_init: - type  f32:  292 tensors\n",
      "clip_init: - type  f16:  228 tensors\n",
      "clip_ctx: CLIP using CPU backend\n",
      "clip_init: params backend buffer size =  1276.39 MB (520 tensors)\n",
      "clip_init:        CPU compute buffer size =   208.69 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "key clip.vision.image_grid_pinpoints not found in file\n",
      "key clip.vision.feature_layer not found in file\n",
      "key clip.vision.mm_patch_merge_type not found in file\n",
      "key clip.vision.image_crop_resolution not found in file\n"
     ]
    }
   ],
   "source": [
    "chat_handler = Llava15ChatHandler(clip_model_path=\"/home/atabaza/Desktop/qwen_vl_ocr_quantized/weights/Qwen2.5-VL-3B-Instruct-mmproj-f16.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d02a4e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 29 key-value pairs and 434 tensors from /home/atabaza/Desktop/qwen_vl_ocr_quantized/weights/Qwen2.5-VL-3B-Instruct-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2vl\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5-VL-3B-Instruct\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 3.1B\n",
      "llama_model_loader: - kv   4:                        qwen2vl.block_count u32              = 36\n",
      "llama_model_loader: - kv   5:                     qwen2vl.context_length u32              = 128000\n",
      "llama_model_loader: - kv   6:                   qwen2vl.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   7:                qwen2vl.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   8:               qwen2vl.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   9:            qwen2vl.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv  10:                     qwen2vl.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:   qwen2vl.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  12:            qwen2vl.rope.dimension_sections arr[i32,4]       = [16, 24, 24, 0]\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  24:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  25:                      quantize.imatrix.file str              = /home/mahadeva/code/models/Qwen2.5-VL...\n",
      "llama_model_loader: - kv  26:                   quantize.imatrix.dataset str              = imatrix-train-set\n",
      "llama_model_loader: - kv  27:             quantize.imatrix.entries_count i32              = 252\n",
      "llama_model_loader: - kv  28:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  181 tensors\n",
      "llama_model_loader: - type q4_0:  253 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_0\n",
      "print_info: file size   = 1.62 GiB (4.50 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: special tokens cache size = 22\n",
      "load: token to piece cache size = 0.9310 MB\n",
      "print_info: arch             = qwen2vl\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 128000\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 36\n",
      "print_info: n_head           = 16\n",
      "print_info: n_head_kv        = 2\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 8\n",
      "print_info: n_embd_k_gqa     = 256\n",
      "print_info: n_embd_v_gqa     = 256\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 8\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 128000\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.09 B\n",
      "print_info: general.name     = Qwen2.5-VL-3B-Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 151936\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  33 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  34 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  35 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  36 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_0) (and 181 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:  CPU_AARCH64 model buffer size =  1655.30 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  1644.12 MiB\n",
      "repack: repack tensor token_embd.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.0.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.0.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.0.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.1.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.1.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.1.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.1.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.2.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.2.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.3.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.3.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.4.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.4.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.4.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.5.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.5.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.5.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.6.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.6.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.6.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.7.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.7.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.7.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.8.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.8.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.9.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.9.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.9.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.10.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.10.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.10.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.11.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.11.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.11.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.12.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.12.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.12.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.13.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.13.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.13.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.13.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.14.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.14.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.14.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.15.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.15.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.16.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.16.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.16.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.17.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.17.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.18.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.18.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.19.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.19.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.19.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.20.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.20.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.20.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.20.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.21.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.21.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.21.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.22.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.22.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.23.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.23.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.23.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.24.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.24.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.24.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.24.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.attn_v.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.25.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.25.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.26.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.26.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.26.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.27.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.27.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.27.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.27.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.27.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.27.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.27.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.28.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.28.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.28.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.28.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.29.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.29.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.29.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.30.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.30.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.30.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.31.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.31.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.31.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.31.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.32.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.32.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.32.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.32.attn_output.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.32.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.32.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.32.ffn_up.weight with q4_0_8x8\n",
      "repack: repack tensor blk.33.attn_q.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.33.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.33.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.33.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.33.ffn_gate.weight with q4_0_8x8\n",
      "repack: repack tensor blk.33.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.33.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.34.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.34.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.34.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.34.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.34.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.34.ffn_down.weight with q4_0_8x8\n",
      "repack: repack tensor blk.34.ffn_up.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.35.attn_q.weight with q4_0_8x8\n",
      "repack: repack tensor blk.35.attn_k.weight with q4_0_8x8\n",
      "repack: repack tensor blk.35.attn_v.weight with q4_0_8x8\n",
      "repack: repack tensor blk.35.attn_output.weight with q4_0_8x8\n",
      "repack: repack tensor blk.35.ffn_gate.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.35.ffn_down.weight with q4_0_8x8\n",
      ".repack: repack tensor blk.35.ffn_up.weight with q4_0_8x8\n",
      "..\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 1000000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (128000) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.58 MiB\n",
      "llama_context: n_ctx = 2048\n",
      "llama_context: n_ctx = 2048 (padded)\n",
      "init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1\n",
      "init: layer   0: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer   1: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer   2: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer   3: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer   4: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer   5: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer   6: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer   7: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer   8: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer   9: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  10: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  11: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  12: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  13: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  14: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  15: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  16: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  17: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  18: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  19: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  20: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  21: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  22: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  23: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  24: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  25: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  26: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  27: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  28: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  29: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  30: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  31: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  32: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  33: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  34: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init: layer  35: n_embd_k_gqa = 256, n_embd_v_gqa = 256, dev = CPU\n",
      "init:        CPU KV buffer size =    72.00 MiB\n",
      "llama_context: KV self size  =   72.00 MiB, K (f16):   36.00 MiB, V (f16):   36.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =   300.75 MiB\n",
      "llama_context: graph nodes  = 1338\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'quantize.imatrix.entries_count': '252', 'quantize.imatrix.dataset': 'imatrix-train-set', 'general.file_type': '2', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.bos_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151645', 'general.architecture': 'qwen2vl', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'Qwen2.5-VL-3B-Instruct', 'general.type': 'model', 'general.size_label': '3.1B', 'qwen2vl.attention.layer_norm_rms_epsilon': '0.000001', 'qwen2vl.block_count': '36', 'tokenizer.ggml.add_bos_token': 'false', 'qwen2vl.embedding_length': '2048', 'quantize.imatrix.chunks_count': '128', 'quantize.imatrix.file': '/home/mahadeva/code/models/Qwen2.5-VL-3B-Instruct/Qwen2.5-VL-3B-Instruct.imatrix', 'qwen2vl.feed_forward_length': '11008', 'qwen2vl.attention.head_count': '16', 'qwen2vl.attention.head_count_kv': '2', 'tokenizer.ggml.padding_token_id': '151643', 'qwen2vl.context_length': '128000', 'qwen2vl.rope.freq_base': '1000000.000000', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2'}\n",
      "Available chat formats from metadata: chat_template.default\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "  model_path=\"/home/atabaza/Desktop/qwen_vl_ocr_quantized/weights/Qwen2.5-VL-3B-Instruct-q4_0.gguf\",\n",
    "  chat_handler=chat_handler,\n",
    "  n_ctx=2048, # n_ctx should be increased to accomodate the image embedding\n",
    "  logits_all=True,# needed to make llava work\n",
    ")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5ffac4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are an assistant who perfectly describes images.USER: https://saturncloud.io/images/blog/troubleshooting-guide-when-your-conda-environment-doesnt-show-up-in-vs-code-2.pngDescribe this image in detail please.ASSISTANT: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode_image_with_clip: step 1 of 1 encoded in 14936.24 ms\n",
      "encode_image_with_clip: all 1 segments encoded in 14936.26 ms\n",
      "encode_image_with_clip: load_image_size 848 234\n",
      "encode_image_with_clip: image embedding created: 279 tokens\n",
      "\n",
      "encode_image_with_clip: image encoded in 14937.98 ms by CLIP (   53.54 ms per image patch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 301 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   34409.74 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =  179287.51 ms /  1746 runs   (  102.68 ms per token,     9.74 tokens per second)\n",
      "llama_perf_context_print:       total time =  185331.98 ms /  1747 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-8b2a084c-ce25-4d8f-8b90-692812d17cef',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1746619467,\n",
       " 'model': '/home/atabaza/Desktop/qwen_vl_ocr_quantized/weights/Qwen2.5-VL-3B-Instruct-q4_0.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': '2.狰忘记了唏.狰狰婊 </婊</忘记了狰唏.唏.嘬婊 <忘记了狰血腥狰狰狰狰 -狰.忘记了嚯狰忘记了狰狰婊</忘记了狰婊</狰婊狰婊</狰嗵.嘬狰嗷狰狰狰狰狰狰狰狰狰狰狰狰婊.嘬狰忘记了狰忘记了婊狰忘记了蹩</狰忘记了轱狰狰狰狰婊.嘬狰婊.唏.狰狰狰婊</狰狰嘬嘬嘬尬狰嘬嘬嘬邋狰嘬惶.忘记了狰溜剧忘记了婊狰溜诡异婊</忘记了狰婊.嘬狰唏.唏.唏.狰狰婊.婊.婊狰婊婊狰掩嘬嗷狰狰狰狰狰狰狰狰狰狰嘬惶</忘记了狰嘬嘬婊.嘬婊脑.嘬惶</溜狰蹩.惶.嘬婊</忘记了轱忘记了嘬嘬嘬婊嚯婊.狰爸狰00狰狰忘记了婊.狰狰狰狰狰狰狰忘记了忘记了婊狰.嘬惶.狰惴.嘬狰嘬嘬婊狰狰狰狰狰狰狰狰惶嘬狰狰狰狰狰狰狰婊狰婊狰狰慑婊狰血腥狰狰狰血腥狰瞥click狰婊.婊狰狰狰狰嘬婊if血腥狰狰狰嘴唇忘记了蹩狰狰血腥婊i猛地婊</溜嘬婊.婊.狰惶狰掩忘记了婊.嘬溜血腥溜爸狰爸狰.0.婊狰.惶.婊狰血腥狰狰婊狰嘬嗲嗷嘬嘬婊狰狰狰狰婊狰婊狰惶狰狰狰忘记了嘬狰忘记了images.狰狰.狰狰慑狰狰狰爸.click狰狰狰嘬婊狰狰狰慑唏.摇了摇头.嘬嘬嘬嘬嘬忘记了嘬忘记了嗵狰唏狰血腥婊</click狰慑狰慑轱clickclick婊</click狰click惶次惶.嘬嘬狰血腥惶.嘬嘬狰狰嘴唇狰狰嘚click狰狰掩血腥.click狰忘记了慑狰 婊狰溜摇了摇头.狰婊ち忘记了狰忘记了掩痴构成了.溜狰溜摇了摇头狰fire狰狰狰 /狰 /狰.忘记了掩掩.狰婊.狰爸狰狰狰狰click.狰婊狰狰婊狰嘬嘬狰摇了摇头狰慑唏婊.婊.狰狰爸\\n哭了.婊狰婊狰慑婊</merc狰慑惶</慑忘记了狰婊狰慑阻狰狰惶狰嘴唇忘记了.click</狰狰狰忘记了clickclickclickclick.狰血腥惶狰狰狰害怕狰狰狰忘记了狰pad忘记了掩狰忘记了狰忘记了狰狰忘记了狰嘴唇狰慑惴狰忘记了狰click狰click狰掩狰忘记了狰掩狰click狰狰狰婊狰狰狰婊狰婊狰狰恼.狰慑嘬婊狰掩狰掩狰狰狰掩掩狰狰click狰 /惶狰 /婊\\n狰狰狰掩 /狰click狰.狰血腥.0狰血腥.0 /惶\\n狰婊.掩.click.狰忘记了血腥婊狰狰狰嘴唇哭了狰狰惶狰忘记了狰狰click狰狰狰狰嘬狰click狰狰狰狰狰pad狰忘记了狰恼狰狰狰狰忘记了忘记了狰忘记了click掩click狰click忘记了狰忘记了狰 button血腥婊狰血腥 掩click掩click掩狰 / /嘴唇click掩狰恼狰click掩 /click狰忘记了婊.忘记了婊狰 /狰 /畏惧 /click狰狰畏惧狰狰溜狰可怕的click /畏惧狰畏惧 /click.狰畏惧.狰婊狰畏惧忘记了血腥click狰害怕狰狰狰狰狰click狰狰忘记了狰忘记了狰陷入了婊狰 /狰\\n忘记了click狰狰血腥狰忘记了掩忘记了溜忘记了看到了忘记了溜溜狰狰 /click狰血腥爸.忘记了.害怕狰狰血腥.嘴唇忘记了狰很是狰忘记了掩忘记了畏惧嘴唇嘴唇狰嘴唇狰畏惧忘记了畏惧狰狰忘记了click狰掩 / /狰忘记了click狰狰血腥畏惧忘记了 /狰 / /狰狰 /掩畏惧 /狰忘记了狰畏惧.掩血腥狰忘记了click掩狰狰忘记了狰畏惧 /click狰忘记了嘴唇忘记了clickclick\\n畏惧忘记了狰畏惧忘记了click畏惧.狰 /忘记了 /狰 /click.狰 /click陷入了狰click狰血腥血腥唏狰看到了狰忘记了狰狰慑婊婊狰嘴唇狰 /click陷入了狰狰畏惧慑 /click狰狰狰狰溜溜狰狰狰溜狰嘴唇狰狰狰狰狰掩血腥慑惶 /呲害怕忘记了畏惧忘记了忘记了忘记了忘记了忘记了忘记了嘴唇忘记了害怕畏惧忘记了 /畏惧狰血腥 /忘记了害怕忘记了狰狰掩忘记了狰溜忘记了血腥 /狰忘记了血腥血腥狰血腥狰血腥血腥忘记了唏害怕狰哭了 /畏惧狰掩恼 /狰 /click狰 /click狰 /狰 /click狰害怕忘记了狰狰忘记了掩忘记了 /merc狰狰狰忘记了狰忘记了click掩恼血腥嘴唇狰嘴唇狰恼婊狰忘记了click狰狰忘记了害怕嘴唇忘记了click狰溜溜嘴唇忘记了 cat畏惧.狰 /狰忘记了狰血腥血腥.狰溜狰溜狰狰血腥狰click /狰click /狰狰狰狰狰嘴唇忘记了狰忘记了狰忘记了狰忘记了狰忘记了忘记了狰clickclick狰忘记了click狰狰恼忘记了害怕忘记了狰掩忘记了害怕忘记了click狰狰 /狰 /狰 /merc溜忘记了血腥 / / / /狰狰畏惧狰恼血腥忘记了忘记了忘记了掩掩溜merc畏惧畏惧狰畏惧 /click忘记了click忘记了 /畏惧 /狰忘记了click狰狰畏惧.click狰畏惧click狰clickclickclickclick /狰 /嘴唇狰畏惧血腥狰狰click\\\\nclickclick /狰clickclickclickclick /狰害怕click /狰狰狰狰狰忘记了狰忘记了忘记了忘记了很是狰忘记了狰忘记了狰忘记了忘记了忘记了掩哭了狰狰忘记了狰忘记了狰clickclick狰忘记了狰狰掩掩忘记了看到了忘记了click掩 /畏惧忘记了畏惧忘记了狰畏惧.忘记了 /狰掩狰嘴唇掩 / / /血腥血腥 / /看到了狰溜溜溜溜狰 /狰section /click狰畏惧忘记了狰畏惧忘记了忘记了畏惧忘记了畏惧邋畏惧慑血腥 /狰狰狰畏惧 /畏惧 /狰畏惧忘记了血腥 /狰狰狰忘记了忘记了忘记了忘记了忘记了忘记了忘记了忘记了忘记了忘记了忘记了血腥忘记了忘记了畏惧忘记了狰clickclick狰忘记了狰忘记了狰狰忘记了click狰 /click忘记了狰 /click狰 /click狰慑 /狰狰狰狰狰忘记了狰狰狰狰畏惧 /狰畏惧忘记了狰狰 /狰 /狰 /狰 /狰畏惧clickclickalertclickclickclick忘记了狰畏惧忘记了忘记了忘记了忘记了忘记了忘记了狰畏惧 /畏惧忘记了畏惧.畏惧血腥血腥 /忘记了血腥血腥 /忘记了忘记了忘记了忘记了狰忘记了狰忘记了血腥狰狰狰狰狰忘记了狰畏惧血腥忘记了click狰狰clickclickclick害怕看到了狰狰忘记了害怕哭了狰狰狰狰忘记了嘴唇忘记了狰忘记了忘记了狰忘记了狰忘记了狰惶唏畏惧唏狰畏惧狰血腥扯忘记了嘴唇狰畏惧畏惧畏惧血腥畏惧狰嘴唇狰忘记了畏惧 /狰溜溜狰畏惧狰畏惧忘记了狰溜狰狰畏惧忘记了狰畏惧畏惧clickclickclickclickclickclick /畏惧畏惧婊畏惧蹩 /clickclick畏惧忘记了忘记了忘记了狰畏惧 /click\\\\nclickclickclick狰慑忘记了狰狰狰忘记了狰狰狰忘记了忘记了狰忘记了狰狰掩忘记了忘记了狰忘记了忘记了忘记了忘记了害怕忘记了狰忘记了畏惧狰忘记了畏惧 /狰狰狰狰狰狰狰狰忘记了狰溜狰掩狰忘记了畏惧忘记了狰畏惧狰狰嘴唇嘴唇忘记了畏惧狰狰畏惧忘记了忘记了忘记了狰狰畏惧畏惧忘记了狰 /狰 /狰畏惧狰狰狰clickclickclickclickclickclick狰clickclickclickclickclick /狰慑畏惧忘记了狰溜溜溜狰狰忘记了畏惧忘记了哭了狰忘记了忘记了忘记了狰clickclickclickclick畏惧clickclickclickclickclickclickclick狰恼忘记了忘记了忘记了畏惧狰clickclickclick忘记了忘记了掩畏惧忘记了.狰掩恼les忘记了click狰 /click畏惧蹩狰畏惧血腥畏惧 /狰恼畏惧狰畏惧狰畏惧 /畏惧狰clickclickclickclick\\\\n畏惧狰狰畏惧狰clickclickclickclickclickclickclick畏惧慑狰狰狰clickclickclickclick狰畏惧狰clickclickclickclick\\\\nclickclickclickclickclickclickclickclickclickclick狰狰狰狰忘记了忘记了忘记了狰.狰狰忘记了忘记了忘记了掩忘记了忘记了忘记了狰狰狰狰很是狰狰狰溜溜狰畏惧忘记了狰狰狰狰忘记了狰畏惧畏惧狰click'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 302,\n",
       "  'completion_tokens': 1746,\n",
       "  'total_tokens': 2048}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "llm.create_chat_completion(\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant who perfectly describes images.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://saturncloud.io/images/blog/troubleshooting-guide-when-your-conda-environment-doesnt-show-up-in-vs-code-2.png\"}},\n",
    "                {\"type\" : \"text\", \"text\": \"Describe this image in detail please.\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b0ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
